{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#loading embeddings and mappings\n",
    "data = np.load(r'C:\\Users\\Darshan\\Music\\raman_kannan\\NLP\\word_embedding\\data\\word_embeddings.npy', mmap_mode='r')\n",
    "with open(r'C:\\Users\\Darshan\\Music\\raman_kannan\\NLP\\word_embedding\\data\\mappings.json') as f:\n",
    "    map_ = json.load(f)\n",
    "data_set = zip(list(map_.values()),data)\n",
    "\n",
    "#Convert to dict for mappings and word_emb to combine\n",
    "dict_emb = {}\n",
    "for k,v in data_set:\n",
    "      dict_emb[k] = v\n",
    "\n",
    "#Read csv dataframe\n",
    "pydf = pd.read_csv(r'C:\\Users\\Darshan\\Music\\raman_kannan\\NLP\\word_embedding\\data\\CodeNetDataFrame_Sub.csv',index_col=0)\n",
    "pydf['vectors'] = pydf['clean_docstring_tokens'].apply(lambda x: np.mean([dict_emb[word] for word in str(x).split() if word in dict_emb],axis=0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Cleaning and Preparation for training \n",
    "df = pydf[pydf['vectors'].apply(lambda x: x.shape == (100,))]\n",
    "array = [df['vectors'].iloc[i] for i in range(len(df))]\n",
    "X = np.array(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Training Completed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_clusters = 5\n",
    "km = KMeans(n_clusters=num_clusters,random_state=42)\n",
    "km.fit(X)\n",
    "clusters = km.labels_.tolist()\n",
    "\n",
    "print('Cluster Training Completed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter human query\n",
      "os memory consumption solution\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "------------------------Code Recommendor----------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>clean_docstring_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24695</th>\n",
       "      <td>def app_trim_memory(self, pid: int or str, lev...</td>\n",
       "      <td>trim memory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19575</th>\n",
       "      <td>def free_memory(self):\\r\\n        \"\"\"Free memo...</td>\n",
       "      <td>free memory signal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3919</th>\n",
       "      <td>def _print_memory(self, memory):\\n        \"\"\"P...</td>\n",
       "      <td>print memory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4821</th>\n",
       "      <td>def collect(self):\\n        \"\"\"\\n        Colle...</td>\n",
       "      <td>collect memory stats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4586</th>\n",
       "      <td>def collect(self):\\n        \"\"\"\\n        Colle...</td>\n",
       "      <td>collect memory stats</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    code  \\\n",
       "24695  def app_trim_memory(self, pid: int or str, lev...   \n",
       "19575  def free_memory(self):\\r\\n        \"\"\"Free memo...   \n",
       "3919   def _print_memory(self, memory):\\n        \"\"\"P...   \n",
       "4821   def collect(self):\\n        \"\"\"\\n        Colle...   \n",
       "4586   def collect(self):\\n        \"\"\"\\n        Colle...   \n",
       "\n",
       "      clean_docstring_tokens  \n",
       "24695           trim memory   \n",
       "19575    free memory signal   \n",
       "3919           print memory   \n",
       "4821    collect memory stats  \n",
       "4586    collect memory stats  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#NLP Part to input the human query with code snippet\n",
    "from scipy.spatial.distance import cosine\n",
    "import re\n",
    "\n",
    "query = str(input('Enter human query\\n'))\n",
    "\n",
    "def tokenize(text):\n",
    "    # obtains tokens with a least 1 alphabet\n",
    "    pattern = re.compile(r'[A-Za-z]+[\\w^\\']*|[\\w^\\']*[A-Za-z]+[\\w^\\']*')\n",
    "    return ' '.join(pattern.findall(text.lower()))\n",
    "\n",
    "def convertToVector(x):\n",
    "      return np.mean([dict_emb[word] for word in x.split() if word in dict_emb],axis=0)\n",
    "\n",
    "def getResults(c,vec):\n",
    "    sub_df = df[df['labels'] == c][['code','vectors','clean_docstring_tokens']]\n",
    "    sub_df['cosine_score'] = sub_df['vectors'].apply(lambda x: 1-cosine(x,vec))\n",
    "    sub_df.sort_values(by='cosine_score',ascending=False,inplace=True)\n",
    "    return sub_df.iloc[0:5]\n",
    "\n",
    "df['labels'] = clusters\n",
    "q = tokenize(query)\n",
    "query_vec = convertToVector(q)\n",
    "query_cluster = km.predict([query_vec])\n",
    "print(query_cluster[0])\n",
    "recom = getResults(query_cluster[0],query_vec)\n",
    "print('------------------------Code Recommendor----------------------------')\n",
    "recom[['code','clean_docstring_tokens']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pyspark code for rdd\n",
    "#convert int to string\n",
    "#key not found in dictionary\n",
    "#how to build tree algorithms like random forests\n",
    "#os memory consumption solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
